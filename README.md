
## Howdy!
This is where I will be keeping all of my work in Natural Language Processing!

## Project 0:
I've written a [summary of NLP](overview-of-nlp.pdf), and what that means to me!

## Project 1: Basic Python
Just a [simple script](project-1/contact-parser.py) that shows off some basic text processing in python. If you would like to learn more, check out my [summary](project-1/summary.md).

## Project 2: POS Tagging and NLTK
Here we have a interactive [guessing game](project-2/guessing-game.py) where the words are taken from a text, tokenized, preprocessed, and the top 50 most common nouns are selected to be options for the guessing game.

## Project 3: WordNet
This notebook plays with some of the functionality of WordNet, a database that links words based on semantic relationships. Read it [here](project-3/wordnet.pdf) and download it [here](project-3/wordnet.ipynb)

## Project 4: N-Grams
Utilzing NLTK I wrote python scripts to create n-grams of some examples of languages [here](project-4/ngram-dictionary.py) and then create a simple language model that identifies if a string of text is likely to be in the analyzed languages: English, French, and Italian. You can run the code [here](project-4/language-finder.py). If you are curious about n-grams, I've written a explanatory [summary](project-4/ngrams-assignment.pdf) talking about what they are, and their applications.

## Sentence Parsing
I've worked a little bit with sentence parses, along with drawing out some parses for my own understanding [here](sentence_parsing.pdf). I must admit English wasn't my favorite subect, so I'm going to let PSGs, Dependency Parse Graphs, and SRL parses do the work for me.

## Netscraping!
Read [this summary](project-5/summary.pdf) to learn about scraping the web. You can access the scripts:
- [Find link and Scrub for Text](project-5/web_crawler.py)
- [Clean Text](project-5/web_crawler2.py)
- [Clean Knowledge Base](project-5/web_crawler3.py)
- [Store indexed knowledge base](project-5/web_crawler4.py)

## Text Classification!
It's easy to underestimate how just taking the frequency of each word in a data set would allow you to classify something about data. The classic example is whether an email is spam or not. But in a [small experiement](project-6/text-classification.pdf), I try and classify whether a line of text.. sounds like Rick from Rick and Morty.
